---
title: "Final"
author: "Vriz Gian Luca"
date: "2023-07-20"
output: pdf_document
---

Firstly we load all the necessary libraries and and functions.

```{r, message=FALSE}
library(exuber)
library(readr)
library(dplyr)
library(car)
library(xtable)
library(dtw)
library(lmtest)
library(tseries)
library(urca)
library(vars)
library(factoextra)
library(ggfortify)
library(modelr)
library(readxl)
library(epiDisplay)
library(blorr)
library(gridExtra)
library(oddsratio)
library(tictoc)
library(Rbeast)
library(psymonitor)
library(bbdetection)
library(devtools)
library(rbmi)
library(dplyr)
library(dqshiny)
library(ggplot2)
library(ghyp)
library(lubridate)
library(strucchange)
library(changepoint)
library(Rssa)
library(tidyr)
library(qcc)
library(corrplot)
library(zoo)
library(cpm)
library(visreg)
library(urca)
library(glmnet)
library(psych)
add_class <- function(x, ...) {
  class(x) <- append(c(...), class(x))
  x
}
```

Then, we load S&P500 daily returns from 10/03/1987 to 30/01/2009 to set up the model used to capture equity-market volatility asymmetry.

```{r}
#install_url('https://cran.r-project.org/src/contrib/Archive/psymonitor/psymonitor_0.0.2.tar.gz')
#install_url('https://cran.r-project.org/src/contrib/Archive/dqshiny/dqshiny_0.0.4.tar.gz')
add_class <- function(x, ...) {
  class(x) <- append(c(...), class(x))
  x
}
#DATA
#https://rdrr.io/rforge/rugarch/man/sp500ret.html
#https://rdrr.io/rforge/rugarch/man/dmbp.html
#data(dmbp)
data("sp500ret")
#auto.arima(sp500ret$SP500RET)
data_garch = sp500ret*100
#GARCH
spec <- ugarchspec(variance.model = list(model = "eGARCH",garchOrder = c(1,1))
                   ,distribution.model="ghyp")
fit = ugarchfit(data = data_garch , spec = spec)

#residuals
garch_resid <- residuals(fit, standardize = FALSE)
# extract standardized residuals
garch_resid_std <- residuals(fit, standardize = TRUE)
# standardized residuals to be used in ugarchsim()
custom_dist = list(
  name = "sample",
  distfit = matrix(garch_resid_std,  ncol = 1)
)

m_ <- fit@model$maxOrder

garch_sim <- ugarchsim(fit, 
                       n.sim = length(garch_resid_std),
                       m.sim = 1, 
                       presigma = tail(fit@fit$sigma, m_),
                       prereturns = tail(sp500ret, m_),
                       # preresiduals ARE NOT standardized:
                       preresiduals = tail(garch_resid, m_),
                       startMethod = "sample",
                       # distfit in custom.dist ARE standardized
                       custom.dist = custom_dist)

# extract simulated series
sp_sim <- garch_sim@simulation$seriesSim
# test
sp_df <- cbind(sp500ret, sim = sp_sim) %>% 
  setNames(c("ret", "sim")) %>% 
  as_tibble(rownames = "date") %>% 
  mutate(date = as.Date(date),
         diff = sim - ret)

egarch<-function(n_sim) {
  #custom_dist = list(
  # name = "sample",
  #distfit = matrix(garch_resid_std[1:n_sim,],  ncol = 1)
  #)
  
  garch_sim <- ugarchsim(fit, 
                         n.sim = n_sim,
                         m.sim = 1, 
                         presigma = tail(fit@fit$sigma, m_),
                         prereturns = tail(sp500ret, m_),
                         # preresiduals ARE NOT standardized:
                         preresiduals = tail(garch_resid, m_),
                         startMethod = "sample",
                         # distfit in custom.dist ARE standardized
                         #custom.dist = custom_dist
                         )
  values = as.numeric(garch_sim@simulation$seriesSim)
  #remove(custom_dist)
  return(values)
}
```

After, we create the bubbles as proposed in the work of Pilips and Shi (2018).

```{r}
bubble <- function(n, te = 0.4 * n, tf = te + 0.2 * n , tr = tf + 0.1*n,
                   c = 1, c1 = 1, c2 = 1, eta = 0.6, alpha = 0.6, beta = 0.5) {
  
  drift <- c*n^(-eta)
  delta <- 1 + c1 * n^(-alpha)
  gamma <- 1 - c2 * n^(-beta)
  y <- 100
  err <- egarch(n)
  
  for (t in 2:n) {
    if (t < te) {
      y[t] <- drift + y[t - 1] + err[t]
    } else if (t >= te & t <= tf) {
      y[t] <- delta * y[t - 1] + err[t]
    } else if (t > tf & t <= tr ) {
      y[t] <- gamma * y[t - 1] + err[t]
    } else {
      y[t] <- drift + y[t - 1] + err[t]
    }
  }
  y %>%
    add_class("sim") 
}


bubble_2 <- function(n,
                    te1 = 0.3 * n, tf1 = te1 + 0.1 * n , tr1 = tf1 + 0.1*n,
                    te2 = 0.8 * n, tf2 = te2 + 0.1 * n , tr2 = tf2 + 0.1*n,
                    c = 1, c1 = 1, c2 = 1, eta = 0.6, alpha = 0.6, beta = 0.7) {
  
  drift <- c*n^(-eta)
  delta <- 1 + c1 * n^(-alpha)
  gamma <- 1 - c2 * n^(-beta)
  y <- 100
  err <- egarch(n)
  
  for (t in 2:n) {
    if (t < te1) {
      y[t] <- drift + y[t - 1] + err[t] # normal
    } else if (t >= te1 & t <= tf1) {
      y[t] <- delta * y[t - 1] + err[t] # bubble1
    } else if (t > tf1 & t <= tr1 ) {
      y[t] <- gamma * y[t - 1] + err[t] # collapse 1
    }  else if (t > tr1 + 1 & t < te2) {
      y[t] <- drift + y[t - 1] + err[t] # normal 2
    }  else if (t >= te2 + 1 & t <= tf2) {
      y[t] <- delta * y[t - 1] + err[t] # bubble 2
    }  else if (t > tf2 + 1 & t <= tr2) {
      y[t] <- gamma * y[t - 1] + err[t] # collapse 2
    } else {
      y[t] <- drift + y[t - 1] + err[t] # normal 3
    }
  }
  y %>%
    add_class("sim")
}

set.seed(000)
time = 100
disturbing <- bubble(time, te = time*0.4, tf= time*0.6, tr = time*0.7, beta = 0.5, alpha = 0.6)
sudden <- bubble(time, time*0.3, tf= time*0.5, tr = time*0.51, beta = 0.15, alpha = 0.6, eta = 0.03)
smooth <- bubble(time, time*0.4, tf= time*0.6, tr = time*0.8, beta = 0.9, alpha = 0.6)
```

In the following there are the plots.

```{r}
autoplot(disturbing)
autoplot(smooth)
autoplot(sudden)
plot(1:99,diff(log(smooth)),type='l')
plot(1:99,diff(log(disturbing)),type='l')
plot(1:99,diff(log(sudden)),type='l')

plot(1:99,abs(diff(log(smooth))),type='l')
plot(1:99,abs(diff(log(disturbing))),type='l')
plot(1:99,abs(diff(log(sudden))),type='l')
```

Now, we can compare the two test using the disturbing bubble.

```{r}
#TEST
sp500ret$data <- row.names(sp500ret)
sp500ret$data <- ymd(sp500ret$data)

series <- abs(diff(log(disturbing)))
              
test <- processStream(series,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
plot(1:length(series),series, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = test$detectionTimes, col = 'blue')
abline(v = test$changePoints, lty = 2, col = 'red')
test

series <- abs(diff(log(smooth)))
              
test <- processStream(series,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
plot(1:length(series),series, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = test$detectionTimes, col = 'blue')
abline(v = test$changePoints, lty = 2, col = 'red')
test

series <- abs(diff(log(sudden)))
              
test <- processStream(series,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
plot(1:length(series),series, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = test$detectionTimes, col = 'blue')
abline(v = test$changePoints, lty = 2, col = 'red')
test

ewma <- numeric(length(series))
ewma[1] <- series[1]^2
lambda <- 0.94
for (i in 2:length(ewma)){
  ewma[i] <- lambda * ewma[i - 1] + (1 - lambda) * series[i]^2
}
plot(ewma, type = "l", xlab = "Observation", ylab = "", bty = "l")

abline(v = test$changePoints, lty = 2)

radf_sim <- radf(ts(disturbing))
autoplot(radf_sim)
datestamp(radf_sim)
```

In the following we will run the simulations.

```{r}
#Distirubing(40,70)

matrix_empty = matrix(nrow = 100, ncol = 1000)

set.seed(002)
cyc = c(1:1000)
DIS <- data.frame(matrix_empty)
for (i in cyc){
  DIS[,i] <- bubble(time, te = time*0.4, tf= time*0.6, tr = time*0.7, beta = 0.5, alpha = 0.6)
}

results_DIS <- list()
ADF_DIS <- list()
for (i in cyc){
  tryCatch({ss <- abs(diff(log(DIS[,i])))
  test <- processStream(ss,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
  results_DIS[[i]]<-test$changePoint
  remove(test)
  radf_sim <- radf(ts(DIS[,i]))
  d<-datestamp(radf_sim)
  ADF_DIS[[i]] <- c(d$series1$Start,d$series1$End)
  remove(d)
  remove(radf_sim)
  print(i)
},error=function(e){cat('ERROR:',conditionMessage(e),'\n')})}
results_DIS
ADF_DIS
correct<-length(results_DIS[lengths(results_DIS) == 3])
correct_ADF<-length(ADF_DIS[lengths(ADF_DIS) == 2])
#962
#849
```

Regarding the disturbing bubble our test recognize correctly 95/100, while ADF 86/100. Our test is also more accurate to detect the end and initial point of the bubble.

```{r}
#Sudden(30,51)
set.seed(003)
SUD <- data.frame(matrix_empty)
for (i in cyc){
  SUD[,i] <- c(bubble(time, time*0.3, tf= time*0.5, tr = time*0.51, beta = 0.15, alpha = 0.6))
}

results_SUD <- list()
ADF_SUD <- list()
for (i in cyc){
  tryCatch({ss <- abs(diff(log(SUD[,i])))
  test <- processStream(ss,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
  results_SUD[[i]]<-test$changePoints
  remove(test)
  radf_sim <- radf(ts(SUD[,i]))
  d<-datestamp(radf_sim)
  ADF_SUD[[i]] <- c(d$series1$Start,d$series1$End)
  remove(d)
  remove(radf_sim)
  print(i)
  },error=function(e){cat('ERROR:',conditionMessage(e),'\n')})}
results_SUD
correct<-length(results_SUD[lengths(results_SUD) == 2])
correct_ADF<-length(ADF_SUD[lengths(ADF_SUD) == 2])
ADF_SUD

#931
#855
```

Regarding the sudden bubble our test recognize correctly 90/100, while ADF 87/100. The test seems to be accurate at the same level.

```{r}
#Smooth(40,80)
set.seed(004)
SMO <- data.frame(matrix_empty)
for (i in cyc){
  SMO[,i] <- c(bubble(time, time*0.4, tf= time*0.6, tr = time*0.8, beta = 0.9, alpha = 0.6))
}

results_SMO <- list()
ADF_SMO <- list()
for (i in cyc){
  tryCatch({ss <- abs(diff(log(SMO[,i])))
  test <- processStream(ss,"Kolmogorov-Smirnov",ARL0=5000, startup=20)
  results_SMO[[i]]<-test$changePoints
  remove(test)
  radf_sim <- radf(ts(SMO[,i]))
  d<-datestamp(radf_sim)
  ADF_SMO[[i]] <- c(d$series1$Start,d$series1$End)
  remove(d)
  remove(radf_sim)
  print(i)
},error=function(e){cat('ERROR:',conditionMessage(e),'\n')})}
results_SMO
correct<-length(results_SUD[lengths(results_SMO) == 3])
correct_ADF<-length(ADF_SUD[lengths(ADF_SMO) == 2])
ADF_SMO
#895
#848
```

Regarding the disturbing bubble our test recognize correctly 87/100, while ADF 83/100. Our test is also more accurate to detect the end and initial point of the bubble.

An alternative and suitable approach is also given in the Bayesian field.

```{r}
series<-abs(diff(log(sudden)))
out = beast(series, season = 'none', precPriorType = 'uniform', mcmc.chains = 10, mcmc.samples = 10000, mcmc.seed = 002, mcmc.burning  = 1000)
plot(out)
```

```{r}
series<-abs(diff(log(disturbing)))
out = beast(series, season = 'none', precPriorType = 'uniform', mcmc.chains = 10, mcmc.samples = 10000, mcmc.seed = 003, mcmc.burning  = 1000)
plot(out)
```

```{r}
series<-abs(diff(log(smooth)))
out = beast(series, season = 'none', precPriorType = 'uniform', mcmc.chains = 10, mcmc.samples = 10000, mcmc.seed = 003, mcmc.burning  = 1000)
plot(out)
```

Now, we will test all three approaches on real data, that is S&P500.

```{r}
#S&P500
SP500 <- as.data.frame(sp500m)
SP500$Date <- row.names(SP500)
SP500$Date <- ymd(SP500$Date)
ggplot(SP500, aes(Date, SP500))+geom_line()
series <- abs(diff(log(SP500$SP500)))
ggplot(SP500[2:840,], aes(Date,series))+geom_line()+ggtitle('S&P500')+xlab('Time')+ylab('Absolute Returns')
```

Firstly the non-parametric test.

```{r}
tic('Time to run')
test <- processStream(series,"Kolmogorov-Smirnov", ARL0=5000, startup=20)
breack<-SP500$Date[test$changePoints]
plot(SP500$Date, SP500$SP500, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = breack, lty = 2, col = 'red')
test

ewma <- numeric(length(series))
ewma[1] <- series[1]^2
lambda <- 0.94
for (i in 2:length(ewma)){
  ewma[i] <- lambda * ewma[i - 1] + (1 - lambda) * series[i]^2
}
plot(SP500$Date[2:840], ewma, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = breack, lty = 2, col = 'red')
toc()
```

Then, the Bayesian test is carried out.

```{r}
tic('Time to run')
series<-ts(series,frequency=12,start=c(1950,1))
out = beast(series, season = 'none', precPriorType = 'uniform', mcmc.chains = 10, mcmc.samples = 10000, mcmc.seed = 002, mcmc.burning  = 1000)
plot(out)
toc()
```

Finally, the SADF test is applied.

```{r}
tic('Time to run')
series<-ts(SP500$SP500,frequency=12,start=c(1950,1))
crit_values_sp<-radf_mc_cv(length(SP500$SP500), seed = 0005)
radf_sim <- radf(series)
datestamp(radf_sim)
autoplot(radf_sim)
toc()
```

How we can see the both Bayesian test and non-parametric test identify the dot-com bubble and the housing bubble. However, only the Bayesian test was able to identify the oil shock, and the black Monday of 1987. On the other hand, the SADFT show similar but not so clear results.

# RENNIX

In the following we found the daily value of the time series of th RENNIX index.

```{r}
renixx <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/Rennix_index.csv")

#DESCRIPTIVE STATISTICS

renixx$date<-ymd(renixx$date)
ggplot(renixx, aes(date, close))+geom_line()+ggtitle('Renixx index')+xlab('Time')+ylab('Absolute Returns')
```

```{r}
#renixx <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/renixx.csv")
renixx$date<-ymd(renixx$date)
df <- renixx %>% dplyr::select(1,3)
df$abs <- abs(c(0,diff(log(df$close))))
ggplot(df, aes(date, abs))+geom_line()
```

From here on we will work with monthly data.

```{r}
Month<- as.Date(cut(df$date, "month"))
renixx_m <- aggregate(close ~ Month, df, mean)
renixx_m<-renixx_m[13:228,]
rownames(renixx_m)<-NULL
renixx_m$abs <- c(0,abs(diff(log(renixx_m$close))))

#Boxcox
#bc_obj<-boxcox(renixx_m$close[135:216])
#distBCMod<-BoxCoxTrans(renixx_m$close)
#distBCMod$lambda
#renixx_m$Renixx <- c(0,diff(predict(bc_obj)))
#renixx_m$Renixx <- c(0,diff(renixx_m$close))
renixx_m$Renixx <- c(0,diff(log(renixx_m$close)))
renixx_m$St<-log(renixx_m$close)
ggplot(renixx_m, aes(Month, abs))+geom_line()+xlab('Time')+ylab('Absolute Returns')+ggtitle('Renixx index')
ggplot(renixx_m, aes(Month, close))+geom_line()+xlab('Time')+ylab('Value')+ggtitle('Renixx index')
```

The CPM is performed to identify bubble behaviours. 

```{r}
test <- processStream(renixx_m$abs,"Kolmogorov-Smirnov", ARL0=5000, startup=42) #20% initial sample
breack<-renixx_m$Month[test$changePoints]
plot(renixx_m$Month, renixx_m$close, type = "l", xlab = "Observation", ylab = "", bty = "l")
abline(v = breack, lty = 2, col = 'red')
test
```

The Bayesian test is carried out also on the Renixx index and the results outlined. 

```{r}
series<-ts(renixx_m$abs,frequency=12,start=c(2005,01,01))
out = beast(series, season = 'none', precPriorType = 'uniform', mcmc.chains = 10, mcmc.samples = 10000, mcmc.seed = 001, mcmc.burning  = 1000)
plot(out)
```

#Explosive behaviours

The BSADF statistics is performed to identify explosive behaviours.

```{r}
set.seed(123)
series <- ts(renixx_m[,c(2,4)],frequency=12,start=c(2005,1))
results<-radf(series[,1])
autoplot(results)+labs(title = "Renixx BSADF")
d_adf<-datestamp(results)
d_adf
```

#Descriptive statistics

Some descriptive statistics are outlined for investigate the relationship among variables.

```{r}
#Oil WTI Futures
Oil <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/Crude Oil WTI Futures Historical Data_2005.csv")
Oil$Date <- mdy(Oil$Date)
Month <- as.Date(cut(Oil$Date, "month"))
Oil_m <- aggregate(Price ~ Month, Oil, mean)

Oil_m$Rate_O<-c(0,diff(log(Oil_m$Price)))
Oil_m$Rate_s<-scale(c(0,diff(Oil_m$Price)),scale=TRUE,center=TRUE)
rownames(Oil_m)<-NULL
ggplot(Oil_m, aes(Month, Price))+geom_line()+ggtitle('Oil price')
```

```{r}
#Geopolitical index 
data_gpr_export <- read_excel("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/data_gpr_daily_recent.xls")
geo_i <- data_gpr_export %>% dplyr::select(6,3)
colnames(geo_i)[1] <- "Date"
geo_i2 <- geo_i[geo_i$Date <= "2022-12-30" & geo_i$Date >= "2005-01-01", ]

Month <- as.Date(cut(geo_i2$Date, "month"))
geo_m <- aggregate(geo_i2$GPRD ~ Month,geo_i2, mean)
colnames(geo_m)[2] <- "GPRH"
geo_m$GPRH<-as.numeric(geo_m$GPRH)

geo_m$Rate_g <- c(0,diff(log(geo_m$GPRH)))
geo_m$St <- scale(c(0,diff(geo_m$GPRH)),scale=TRUE,center=TRUE)
ggplot(geo_m, aes(Month, GPRH))+geom_line()+ggtitle('Geopolitical index')
```

```{r}
#MSCI_WORLD
MSCI<-read_csv("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/MSCI World_2005.csv")
MSCI_2 <- MSCI %>% dplyr::select(6,2)
MSCI_2$Date <- format(as.Date(MSCI_2$Date,format='%m/%d/%y'), "%m/%d/%Y")
MSCI_2$Date <- mdy(MSCI_2$Date)
Month <- as.Date(cut(MSCI_2$Date, "month"))
MSCI_2$Close<-as.numeric(MSCI_2$Close)
MSCI_m <- aggregate(Close ~ Month, MSCI_2, mean)
colnames(MSCI_m)[2] <- "Price"
MSCI_m$Price<-as.numeric(MSCI_m$Price)

MSCI_m$Rate_ms <- c(0,diff(log(MSCI_m$Price)))
MSCI_m$Rate_s <- scale(c(0,diff(MSCI_m$Price)),scale=TRUE,center=TRUE)
ggplot(MSCI_m, aes(Month, Price))+geom_line()+ggtitle('Global MSCI index')
```

```{r}
#Policy risk and Financial risk
GEPUCURRENT <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/GEPUCURRENT.csv")
GEPUCURRENT$DATE<-ymd(GEPUCURRENT$DATE)
GEPUCURRENT<-GEPUCURRENT[97:312,]
rownames(GEPUCURRENT) <- NULL
GEPUCURRENT$Rate<-c(0,diff(log(GEPUCURRENT$GEPUCURRENT)))
GEPUCURRENT$Rate_s<-scale(c(0,diff(GEPUCURRENT$GEPUCURRENT)),scale=TRUE,center=TRUE)

Fs<-read.csv("~/GitHub/PhD-Thesis/Green bubbles/Economic variables/Fs.csv")
Month<- as.Date(cut(ymd(Fs$Date), "month"))
Fs_m <- aggregate(OFR.FSI ~ Month, Fs, mean)
Fs_m<-Fs_m[61:276,]
Fs_m$Rate<-c(0,diff(Fs_m$OFR.FSI))
Fs_m$Rate_s<-scale(c(0,diff(Fs_m$OFR.FSI)),scale=TRUE,center=TRUE)
ggplot(Fs_m, aes(Month, OFR.FSI))+geom_line()+ggtitle('Financial Stability')
ggplot(GEPUCURRENT, aes(DATE, GEPUCURRENT))+geom_line()+ggtitle('Policy Uncertainty')
```

# What happens inside different phases

```{r}
#Financial variables
Finance<-renixx_m[,c(1,4)]
colnames(Finance)[2] <- "Renixx"

Finance$Oil_p<-Oil_m$Rate_O

Finance$MSCI<-MSCI_m$Rate_ms

Finance$EPU<-GEPUCURRENT$Rate

Finance$OFR<-Fs_m$Rate

Finance$Geo_i<-geo_m$Rate_g

Finance_2<-renixx_m[,c(1,2)]
colnames(Finance_2)[2] <- "Renixx"
Finance_2$Renixx<-scale(Finance_2$Renixx,center=TRUE,scale=TRUE)
Finance_2$Oil_p<-scale(Oil_m$Price,center=TRUE,scale=TRUE)
Finance_2$Geo_i<-scale(geo_m$GPRH,center=TRUE,scale=TRUE)
Finance_2$MSCI<-scale(MSCI_m$Price,center=TRUE,scale=TRUE)
Finance_2$EPU<-scale(GEPUCURRENT$GEPUCURRENT,center=TRUE,scale=TRUE)
Finance_2$OFR<-scale(Fs_m$OFR.FSI,center=TRUE,scale=TRUE)


ggplot() + 
  geom_line(data = Finance_2, aes(x = Month, y = Renixx, color = "Rennix")) +
  geom_line(data = Finance_2, aes(x = Month, y = MSCI, color = "Global MSCI")) +
  geom_line(data = Finance_2, aes(x = Month, y = Geo_i, color = "Geopolitical index")) +
  geom_line(data = Finance_2, aes(x = Month, y = Oil_p, color = "Oil price")) +
  geom_line(data = Finance_2, aes(x = Month, y = EPU, color = "EPU")) +
  geom_line(data = Finance_2, aes(x = Month, y = OFR, color = "OFR")) +
  xlab('data_date') + ggtitle('Financial time series')+
  ylab('Monthly series')+ scale_color_manual(name='Series',
                     breaks=c('Rennix', 'Global MSCI','Geopolitical index','Oil price','EPU','OFR'),
                     values=c('Rennix'='green', 'Global MSCI'='red','Geopolitical index'='blue','Oil price'='brown','EPU'='orange','OFR'='grey'))
```

```{r}
#Text variables
green_e <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Green_energy.csv", header=TRUE, comment.char="#")[13:228,]
green_e$Time<-ymd(green_e$Time)
green_e$Rate<-c(0,diff(log(green_e$Absolute.Google.Search.Volume)))
Climate<-as.data.frame(green_e[,1]) 
colnames(Climate)[1] <- "Date"
Climate$Green_e<-green_e[,4]

Tech_d <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/New Technology.csv", header=TRUE, comment.char="#")[13:228,]
Tech_d$Rate<-c(0,diff(log(Tech_d$Absolute.Google.Search.Volume)))
Climate$Tech<-Tech_d[,4]

Energy_i <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Energy_index.csv", header=TRUE, comment.char="#")[13:228,]

Energy_i<-Energy_i[,c(1,2,3)]
Energy_i$Rate<-c(0,diff(log(Energy_i$Absolute.Google.Search.Volume)))
Climate$Energy_i<-Energy_i[,4]

global_w <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Global_warming.csv", header=TRUE, comment.char="#")[13:228,]
global_w$Rate<-c(0,diff(log(global_w$Absolute.Google.Search.Volume)))
Climate$Warming<-global_w[,4]

Natural <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Natural_disasters.csv", header=TRUE, comment.char="#")[13:228,]
Natural$Rate<-c(0,diff(log(Natural$Absolute.Google.Search.Volume)))
Climate$Natural<-Natural[,4]

Carbon_p <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Carbon_price.csv", header=TRUE, comment.char="#")[13:228,]
Carbon_p$Rate<-c(0,diff(log(Carbon_p$Absolute.Google.Search.Volume)))
Climate$Carbon_p<-Carbon_p[,4]

Tax <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Carbon_tax.csv", header=TRUE, comment.char="#")[13:228,]
Tax$Rate<-c(0,diff(log(Tax$Absolute.Google.Search.Volume)))
Climate$Tax<-Tax[,4]

Energy_s <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Energy_shares.csv", header=TRUE, comment.char="#")[13:228,]
Energy_s$Rate<-c(0,diff(log(Energy_s$Absolute.Google.Search.Volume)))
Climate$Energy_s<-Energy_s[,4]

Climate_2<-as.data.frame(green_e[,1]) 
colnames(Climate_2)[1] <- "Date"

Climate_2$Green_e<-scale(as.numeric(green_e[,3]),center = TRUE,scale=TRUE)
Climate_2$Tech<-scale(as.numeric(Tech_d[,3]),center = TRUE,scale=TRUE)
Climate_2$Warming<-scale(as.numeric(global_w[,3]),center = TRUE,scale=TRUE)
Climate_2$Energy_i<-scale(as.numeric(Energy_i[,3]),center = TRUE,scale=TRUE)
Climate_2$Natural<-scale(as.numeric(Natural[,3]),center = TRUE,scale=TRUE)
Climate_2$Carbon_p<-scale(as.numeric(Carbon_p[,3]),center = TRUE,scale=TRUE)
Climate_2$Energy_s<-scale(as.numeric(Energy_s[,3]),center = TRUE,scale=TRUE)
Climate_2$Energy_T<-scale(as.numeric(Energy_s[,3]+Energy_i[,3])/2,center = TRUE,scale=TRUE)
Climate_2$Energy_s<-scale(as.numeric(Energy_s[,3]),center = TRUE,scale=TRUE)
Climate_2$Tax<-scale(as.numeric(Tax[,3]),center = TRUE,scale=TRUE)

ggplot() + 
  geom_line(data = Climate_2, aes(x = Date, y = Green_e, color = "Green Energy")) +
  geom_line(data = Climate_2, aes(x = Date, y = Energy_i, color = "Energy Index")) +
  geom_line(data = Climate_2, aes(x = Date, y = Warming, color = "Global Warming")) +
  geom_line(data = Climate_2, aes(x = Date, y = Natural, color = "Natural Disasters"))+ 
  geom_line(data = Climate_2, aes(x =Date, y = Carbon_p, color = "Carbon price")) +
 geom_line(data = Climate_2, aes(x =Date, y = Energy_s, color = "Energy shares")) +
  geom_line(data = Climate_2, aes(x =Date, y = Tax, color = "Carbon Tax")) +
  geom_line(data = Climate_2, aes(x =Date, y = Tech, color = "New Technology")) +
  xlab('data_date') + ggtitle('Text-time series')+
  ylab('Monthly series')+ scale_color_manual(name='Series',
                     breaks=c('Green Energy', 'Global Warming','Natural Disasters','Carbon price',"New Technology",'Energy Index','Green Bond','Carbon Emissions','Energy shares','Carbon Tax'),
                     values=c('Green Energy'='green','Global Warming'='red', 'Natural Disasters'='gold','Carbon price'='brown','New Technology'='purple','Energy Index'='blue','Carbon Emissions'='orange','Energy shares'='violet','Carbon Tax'='grey'))
```

```{r}
corrplot(cor(Climate[c(1:216),c(2:8)],method='kendall'), hclust.method='ward.D2',addrect=3,rect.col = 'gold',rect.lwd=4,order='hclust',diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
corrplot(cor(Finance[c(1:216),(2:length(Finance[1,]))],method='kendall'), hclust.method='ward.D2',addrect=3,rect.col = 'gold',rect.lwd=4,order='hclust',diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
```

```{r}
Finance$Merge<-format(as.Date(Finance$Month), "%Y-%m")
Climate$Merge<-format(as.Date(Climate$Date), "%Y-%m")

Data<-merge(Finance,Climate,by="Merge")
Data<- Data[c(2:216),-c(1,9)]
rownames(Data) <- NULL

corrplot(cor(Data[1:133,c(2:length(Data[1,]))],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
corrplot(cor(Data[134:177,c(2:length(Data[1,]))],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
corrplot(cor(Data[178:215,c(2:length(Data[1,]))],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
corrplot(cor(Data[,c(2:length(Data[1,]))],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')
#"2016-03-01" "2019-10-01"
```

```{r}
Climate_2<-Climate_2[c('Date','Green_e','Tech','Warming','Natural','Carbon_p','Energy_i','Energy_s','Tax')]
Finance_2$Merge<-format(as.Date(Finance_2$Month), "%Y-%m")
Climate_2$Merge<-format(as.Date(Climate_2$Date), "%Y-%m")

Data_or<-merge(Finance_2,Climate_2,by="Merge")
Data_or<- Data_or[,-c(1,9)]
rownames(Data_or) <- NULL
```

Then, the ADF and the KPSS proposed to test for stationarity in the log-returns.

```{r}
#Stationary assumption of log-returns

summary(ur.df(Climate[,"Carbon_p"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Green_e"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Tech"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Warming"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Natural"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[3:216,"Tax"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Energy_s"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Climate[,"Energy_i"], type = "drift",lags = 12, selectlags = "AIC"))

summary(ur.df(Finance[,"Oil_p"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Finance[,"Renixx"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Finance[3:216,"MSCI"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Finance[,"EPU"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Finance[3:216,"OFR"], type = "drift",lags = 12, selectlags = "AIC"))
summary(ur.df(Finance[,"Geo_i"], type = "drift",lags = 12, selectlags = "AIC"))

kpss.test(Climate[,"Carbon_p"])
kpss.test(Climate[,"Tech"])
kpss.test(Climate[,"Natural"])
kpss.test(Climate[,"Green_e"])
kpss.test(Climate[,"Warming"])
kpss.test(Climate[,"Energy_s"])
kpss.test(Climate[,"Tax"])
kpss.test(Climate[,"Energy_i"])
kpss.test(Finance[,"Oil_p"])
kpss.test(Finance[,"Renixx"])
kpss.test(Finance[,"MSCI"])
kpss.test(Finance[,"Geo_i"])
kpss.test(Finance[,"EPU"])
kpss.test(Finance[,"OFR"])
bptest(lm(Finance[,"OFR"] ~ seq_along(Finance[,"OFR"])))
```

#Cluster analysis

A cluster analysis is also performed using a dynamic time window method to investigate similarites among all time series. 

```{r}
ratesa<-Data_or[1:134,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")

ratesa<-Data_or[135:178,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")

ratesa<-Data_or[179:216,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")


ratesa<-Data_or[,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")
```

#Factor analysis

A factor analysis on the stationary time series is carried out outlined the presence of two distinc factors. A financial one and a climate one. 

```{r}
#https://rpubs.com/Pun_/Exploratory_factor_Analysis
#Promax Rotation. An oblique rotation, which allows factors to be correlated. This rotation can be calculated more quickly than a direct oblimin rotation, so it is useful for large datasets.

Data_FA<-renixx_m[,c(1,4)]
colnames(Data_FA)[2] <- "Renixx"
Data_FA$Renixxlag<-lag(Finance$Renixx)
Data_FA$Oil_lagp<-lag(Finance$Oil_p)
Data_FA$MSCIlag<-lag(Finance$MSCI)
Data_FA$EPUlag<-lag(Finance$EPU)
Data_FA$Geo_lagi<-lag(Finance$Geo_i)
Data_FA$OFRlag<-lag(Finance$OFR)

Data_FA$Warminglag<-lag(global_w[,4])
Data_FA$Naturallag<-lag(Natural[,4])
Data_FA$Carbon_plag<-lag(Carbon_p[,4])
Data_FA$Techlag<-lag(Tech_d[,4])
Data_FA$Energy_ilag<-lag(Energy_i[,4])
Data_FA$Green_elag<-lag(green_e[,4])
Data_FA$Taxlag<-lag(Tax[,4])
Data_FA$Energy_slag<-lag(Energy_s[,4])

Data_FA<-Data_FA[c(2:216),]
rownames(Data_FA)<-NULL

KMO(Data_FA[,c(3:length(Data_FA[1,]))])
Data_factor <-Data_FA[,c(3:length(Data_FA[1,]))]
Data_factor <-Data_factor[,KMO(Data_factor)$MSAi>0.7]
base::round(KMO(Data_factor)$MSA,2)

cortest.bartlett(Data_factor)

ev <- eigen(cor(Data_factor)) # get eigenvalues
ev$values

scree(Data_factor, pc=FALSE)

fa.parallel(Data_factor)

Nfacs <- 2 # This is for four factors. You can change this as needed.

fit <- fa(Data_factor, Nfacs, rotate = "none",fm="promax")

print(fit, digits=3, cutoff=0.3, sort=TRUE)

loads <- fit$loadings

fa.diagram(loads)

dim(fit$loadings)

fs <- factor.scores(Data_factor, fit)   
fs <- fs$scores
Data_FA$Factor_climate<-fs[,1]
Data_FA$Factor_finance<-fs[,2]

Data_FA$OFR<-Fs_m$Rate[2:216]
cor(Data_FA$Factor_climate[c(1:133)],Data_FA$OFR[c(1:133)],method='kendal')
cor(Data_FA$Factor_climate[c(134:177)],Data_FA$OFR[c(134:177)],method='kendal')
cor(Data_FA$Factor_climate[c(178:215)],Data_FA$OFR[c(178:215)],method='kendal')

cor(Data_FA$Factor_finance[c(1:133)],Data_FA$OFR[c(1:133)],method='kendal')
cor(Data_FA$Factor_finance[c(134:177)],Data_FA$OFR[c(134:177)],method='kendal')
cor(Data_FA$Factor_finance[c(178:215)],Data_FA$OFR[c(178:215)],method='kendal')

var_fact<-VAR(Data_FA[c(1:215),c('OFRlag','Factor_climate')],season = 12,p=1,type='const')
summary(var_fact)
set.seed(6)
causality(var_fact, cause = 'Factor_climate', boot=TRUE, boot.runs=1000)
causality(var_fact, cause = 'OFRlag', boot=TRUE, boot.runs=1000)
```

#PCA Bubble

A principal component analysis on the three different stages of the Renixx index outlines remarkable disitnctions among the variables over time.

```{r}
library(bestNormalize)
library(caret)
library(Rssa)
library(lattice)
library(fANCOVA)
library(stats)
library(HoRM)
library(WaveletComp)
library(parameters)
library(TSA)
library(fGarch)
library(fpp)
library(vars)
library(BVAR)
library(Metrics)
library(aTSA)
library(tsoutliers)
library(tsDyn)
library(goft)
library(bruceR)
library(plotmo)
library(grid)
library(factoextra)
library(ggfortify)
library(tvReg)
```

```{r}
#PCA

Data_PCA<-Data_FA[c(1:133),c(3:16)]
data_PCA<-prcomp(Data_PCA,scale =TRUE,center = TRUE)

autoplot(data_PCA,loadings=TRUE,loadings.colour='Blue',loadings.label=TRUE,loadings.labal.size=3,data=Data_FA[c(1:133),],colour='Renixx')

stats<-summary(data_PCA)
importance<-stats$importance[3,]
importance[importance<=0.85]

fviz_contrib(data_PCA, choice = "var", axes = 1, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 2, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 5, top = 3)

x_results<-as.data.frame(data_PCA$x)
PCA_data<-as.data.frame(x_results$PC1)
names(PCA_data)[1]<-'PC1'
PCA_data$PC2<-x_results$PC2
PCA_data$PC3<-x_results$PC3
PCA_data$PC4<-x_results$PC4
PCA_data$PC6<-x_results$PC6
PCA_data$PC7<-x_results$PC7

cor<-matrix(ncol=1,nrow=length(PCA_data[,]))
for(i in 1:length(PCA_data[,])){
  cor[i]<-cor(Data_FA$Renixx[1:133],(PCA_data[,i]),method='kendal')
}

plot(abs(cor))
```

```{r}
Data_PCA<-Data_FA[c(134:177),c(3:16)]
data_PCA<-prcomp(Data_PCA,scale =TRUE,CENTER=TRUE)

autoplot(data_PCA,loadings=TRUE,loadings.colour='Blue',loadings.label=TRUE,loadings.labal.size=3,data=Data_FA[c(134:177),],colour='Renixx')

stats<-summary(data_PCA)
importance<-stats$importance[3,]
importance[importance<=0.85]

fviz_contrib(data_PCA, choice = "var", axes = 1, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 2, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 3, top = 3)

x_results<-as.data.frame(data_PCA$x)
PCA_data<-as.data.frame(x_results$PC1)
names(PCA_data)[1]<-'PC1'
PCA_data$PC2<-x_results$PC2
PCA_data$PC3<-x_results$PC3
PCA_data$PC4<-x_results$PC4
PCA_data$PC6<-x_results$PC6

cor<-matrix(ncol=1,nrow=length(PCA_data[,]))
for(i in 1:length(PCA_data[,])){
  cor[i]<-cor(Data_FA$Renixx[134:177],(PCA_data[,i]),method='kendal')
}

plot(abs(cor))
```

```{r}
Data_PCA<-Data_FA[c(178:215),c(3:16)]
data_PCA<-prcomp(Data_PCA,scale =TRUE,center=TRUE)

autoplot(data_PCA,loadings=TRUE,loadings.colour='Blue',loadings.label=TRUE,loadings.labal.size=3,data=Data_FA[c(178:215),],colour='Renixx')

stats<-summary(data_PCA)
importance<-stats$importance[3,]
importance[importance<=0.85]

fviz_contrib(data_PCA, choice = "var", axes = 1, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 2, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 4, top = 3)

x_results<-as.data.frame(data_PCA$x)
PCA_data<-as.data.frame(x_results$PC1)
names(PCA_data)[1]<-'PC1'
PCA_data$PC2<-x_results$PC2
PCA_data$PC3<-x_results$PC3
PCA_data$PC4<-x_results$PC4
PCA_data$PC6<-x_results$PC6

cor<-matrix(ncol=1,nrow=length(PCA_data[,]))
for(i in 1:length(PCA_data[,])){
  cor[i]<-cor(Data_FA$Renixx[178:215],(PCA_data[,i]),method='kendal')
}

plot(abs(cor))
```

# Explosive/No-explosive

All previous methodologies are carried out considering also explosive/no-explosive behaviours.

```{r}
Data_FA$dummy <- as.factor(ifelse(Data_FA$Month >= "2007-07-01" & Data_FA$Month <= "2007-08-01"|Data_FA$Month >= "2007-10-01" & Data_FA$Month <= "2008-01-01"|Data_FA$Month >= "2015-04-01" & Data_FA$Month <= "2015-06-01"|Data_FA$Month >= "2019-12-01" & Data_FA$Month <= "2020-03-01"|Data_FA$Month >= "2020-07-01" & Data_FA$Month <= "2021-05-01"|Data_FA$Month >= "2021-11-01" & Data_FA$Month <= "2021-12-01", 1,0))

Data_exp<-Data_FA[Data_FA$dummy==1,c(4:length(Data_FA)-2)]
Data_noexp<-Data_FA[Data_FA$dummy==0,c(4:length(Data_FA)-2)]
```

```{r}
d_adf
data_PCA<-prcomp(Data_exp[,c(4:length(Data_exp)-2)],scale =TRUE)

autoplot(data_PCA,loadings=TRUE,loadings.colour='Blue',loadings.label=TRUE,loadings.labal.size=3,data=Data_exp,colour='Renixx')

stats<-summary(data_PCA)
importance<-stats$importance[3,]
importance[importance<=0.85]

fviz_contrib(data_PCA, choice = "var", axes = 1, top = 3)
fviz_contrib(data_PCA, choice = "var", axes = 2, top = 3)

x_results<-as.data.frame(data_PCA$x)
PCA_data<-as.data.frame(x_results$PC1)
names(PCA_data)[1]<-'PC1'
PCA_data$PC2<-x_results$PC2
PCA_data$PC3<-x_results$PC3
PCA_data$PC4<-x_results$PC4
PCA_data$PC5<-x_results$PC5

cor<-matrix(ncol=1,nrow=length(PCA_data[,]))
for(i in 1:length(PCA_data[,])){
  cor[i]<-cor(Data_exp$Renixx,(PCA_data[,i]),method='kendal')
}

plot(abs(cor))

corrplot(cor(Data_exp[,c(3:length(Data_exp)-2)],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')

corrplot(cor(Data_noexp[,c(3:length(Data_noexp)-2)],method='kendall'),diag=FALSE, method='square',bg='black',col=c('lightblue','green'),tl.col='black')


ratesa<-Data_or[Data_FA$dummy==1,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")


ratesa<-Data_or[Data_FA$dummy==0,]
ratesa$Month<-NULL;
ratesa<-ratesa
tr<-t(ratesa);
distMatrix <- dist(tr, method='DTW');
#hierarchical clustering
hc <- hclust(distMatrix,method="average")
plot(hc, main="Clustering by average")

cor(Data_exp$Factor_climate,Data_exp$Renixx,method='kendal')
cor(Data_exp$Factor_finance,Data_exp$Renixx,method='kendal')
cor(Data_exp$Factor_climate,Data_exp$OFRlag,method='kendal')
cor(Data_exp$Factor_finance,Data_exp$OFRlag,method='kendal')

tvLM<-tvLM(Renixx ~ 0 +Renixxlag+Energy_ilag+Energy_slag+Carbon_plag+Green_elag+Taxlag+MSCIlag+Warminglag+Naturallag+Techlag, data = Data_FA, bw = 0.5)
plot(Data_FA$Month, tvLM$coefficients[,1],xlab='Time',ylab='Renixx previous values correlation')
plot(Data_FA$Month, tvLM$coefficients[,2],xlab='Time',ylab='Energy index correlation')
```

#What drives bubbles in terms of text analysis.

In the following some econometric models are used to investigate the main drivers among the two explosive bubble as well as explosive moments.

#Bubbles dynamics

For bubble dynamics a multi-logit model is performed to examine possible bubble drivers. The dataset, composed by standardized search volume index, is randomly split. This setup is chosen to allow a comphrensive test set.  

```{r}
#Dummy
Data_2<-renixx_m[c(2:216),c(1,4)]
colnames(Data_2)[2] <- "Renixx"

Data_2$dummy_Renixx  <- cut(Data_2$Month,
              breaks= as.Date(c("2005-01-01","2016-03-01","2019-11-01",'2023-01-01')),
              labels=c('Clean-Tech', 'No-bubble', 'Climate'))

Data_2$dummy_Renixx<-relevel(Data_2$dummy_Renixx, ref = 'No-bubble')

Data_2$Energy_ilag<-lag(Data_or$Energy_i)[2:216]
Data_2$Warminglag<-lag(Data_or$Warming)[2:216]
Data_2$Naturallag<-lag(Data_or$Natural)[2:216]
Data_2$Green_elag<-lag(Data_or$Green_e)[2:216]
Data_2$Carbon_plag<-lag(Data_or$Carbon_p)[2:216]
Data_2$Techlag<-lag(Data_or$Tech)[2:216]
Data_2$Energy_slag<-lag(Data_or$Energy_s)[2:216]
Data_2$Taxlag<-lag(Data_or$Tax)[2:216]

Data_2<-Data_2[,-c(1,2)]

rownames(Data_2) <- NULL

str(Data_2)
```

```{r}
p1 <- ggplot(Data_2, aes(x=Green_elag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx))+ labs(fill='Bubble')+
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Green energy')
p2 <- ggplot(Data_2, aes(x=Naturallag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4)+
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Natural disasters')+ labs(fill='Bubble') 
p3 <- ggplot(Data_2, aes(x=Carbon_plag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) + xlab('Carbon price')+
      guides(color = "none")+ geom_density(alpha=0.4) + labs(fill='Bubble') 
p4 <- ggplot(Data_2, aes(x=Energy_ilag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Energy index')+ labs(fill='Bubble') 
p5 <- ggplot(Data_2, aes(x=Warminglag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Global warming')+ labs(fill='Bubble')
p6 <- ggplot(Data_2, aes(x=Techlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('New technology')+ labs(fill='Bubble')
p7 <- ggplot(Data_2, aes(x=Energy_slag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Energy shares')+ labs(fill='Bubble')
p8 <- ggplot(Data_2, aes(x=Taxlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Carbon tax')+ labs(fill='Bubble')
grid.arrange(p1, p2, p3, p4, p5, p6,p7,p8, ncol=4,top = textGrob("RENIXX Index",gp=gpar(fontsize=20,font=3)))
```

```{r}
smp_size <- floor(0.7 * nrow(Data_2))
Data_multi<-Data_2
Data_multi$Time<-c(1:215)
##set the seed to make your partition reproducible
set.seed(1239)
train_ind <- sample(seq_len(nrow(Data_multi)), size = smp_size)

train.data <- Data_multi[train_ind, ]
vali.data <- Data_multi[-train_ind, ] 
#Dumy code categorical predictor variables
x <- model.matrix(dummy_Renixx ~., train.data)[,-1]
y<-train.data$dummy_Renixx
```

```{r}
#Find the best lambda using cross-validation
set.seed(123) 
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "multinomial")
#Fit the final model on the training data
model <- glmnet(x, y, alpha = 1, family = "multinomial",
                lambda = cv.lasso$lambda.min)
#Display regression coefficients
coef(model)
# Display regression coefficients
plot_glmnet(cv.lasso$glmnet.fit, label=TRUE,nresponse = 3)
plot_glmnet(cv.lasso$glmnet.fit, label=TRUE,nresponse = 2)
plot_glmnet(cv.lasso$glmnet.fit, label=TRUE,nresponse = 1)
```

```{r}
set.seed(123)
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "multinomial")
plot(cv.lasso)
```

```{r}
cv.lasso$lambda.min
coef(cv.lasso, cv.lasso$lambda.min)
```

```{r}
lasso.model <- glmnet(x, y, alpha = 1, family = "multinomial",
                      lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(dummy_Renixx ~., vali.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test)
predicted.classes <- probabilities

for(i in 1:length(probabilities[,1,])){
  for(j in 1:length(probabilities[1,,])){
predicted.classes[i,j,] <- ifelse(probabilities[i,j,]==max(probabilities[i,,]), 1, 0)
}
}

predicted.classes

# 5 errors on 65
```

#Elastic-net regularized logistic regression for explosive moments

For explosive behaviors a elastic-net regularized logistic regression is adopted to anticipate possible explosive moments. In this vew a time dependent cross validation approach is adopted and the test set composed on the last 25 observation of the samlple.

```{r}
df <- renixx %>% dplyr::select(1,5)
Month<- as.Date(cut(df$date, "month"))
renixx_m$low <- aggregate(low ~ Month, df, mean)[13:228,2]

df <- renixx %>% dplyr::select(1,4)
Month<- as.Date(cut(df$date, "month"))
renixx_m$high <- aggregate(high ~ Month, df, mean)[13:228,2]
```

```{r}
Data_2$dummy_Renixx<- as.factor(ifelse(Data_FA$Month >= "2007-07-01" & Data_FA$Month <= "2007-08-01"|Data_FA$Month >= "2007-10-01" & Data_FA$Month <= "2008-01-01"|Data_FA$Month >= "2015-04-01" & Data_FA$Month <= "2015-06-01"|Data_FA$Month >= "2019-12-01" & Data_FA$Month <= "2020-03-01"|Data_FA$Month >= "2020-07-01" & Data_FA$Month <= "2021-05-01"|Data_FA$Month >= "2021-11-01" & Data_FA$Month <= "2021-12-01", 'Explosive','No-Explosive'))
Data_2$dummy_Renixx
Data_2$dummy_Renixx<-relevel(Data_2$dummy_Renixx, ref = 'No-Explosive')

Data_2$diff_Enii<-lag(scale((Data_or$Renixx-Data_or$Energy_i),center=TRUE,scale=TRUE))[2:216]
Data_2$MSCIlag<-lag(MSCI_m$Price)[2:216]
Data_2$Oillag<-lag(Oil_m$Price)[2:216]
Data_2$EPUlag<-lag(GEPUCURRENT$GEPUCURRENT)[2:216]
Data_2$OFRlag<-lag(Fs_m$OFR.FSI)[2:216]

Data_2$Oillag<-scale(c(0,diff(Data_2$Oillag)),center = TRUE,scale=TRUE)
Data_2$EPUlag<-scale(c(0,diff(Data_2$EPUlag)),center = TRUE,scale=TRUE)
Data_2$MSCIlag<-scale(c(0,diff(Data_2$MSCIlag)),center = TRUE,scale=TRUE)
Data_2$OFRlag<-scale(c(0,diff(Data_2$OFRlag)),center = TRUE,scale=TRUE)
```

```{r}
p1 <- ggplot(Data_2, aes(x=Green_elag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx))+ labs(fill='Green bubble')+
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Green Energy')
p2 <- ggplot(Data_2, aes(x=Naturallag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4)+
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Natural disaster')+ labs(fill='Green bubble') 
p3 <- ggplot(Data_2, aes(x=Carbon_plag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) + xlab('Carbon price')+
      guides(color = "none")+ geom_density(alpha=0.4) + labs(fill='Green bubble') 
p4 <- ggplot(Data_2, aes(x=Energy_ilag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Energy Index')+ labs(fill='Green bubble') 
p5 <- ggplot(Data_2, aes(x=Warminglag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('Global Warming')+ labs(fill='Green bubble')
p6 <- ggplot(Data_2, aes(x=Techlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('New Technology')+ labs(fill='Green bubble')
p7 <- ggplot(Data_2, aes(x=Energy_slag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Energy Shares')+ labs(fill='Green bubble')
p8 <- ggplot(Data_2, aes(x=Taxlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Carbon Tax')+ labs(fill='Green bubble')
grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, ncol=2,top = textGrob("Renixx Global Index Explosive Evets",gp=gpar(fontsize=20,font=3)))
```

```{r}
p9 <- ggplot(Data_2, aes(x=OFRlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('OFR')+ labs(fill='Green bubble')
p10 <- ggplot(Data_2, aes(x=MSCIlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
      guides(color = "none")+ geom_density(alpha=0.4) + xlab('MSCI')+ labs(fill='Green bubble')
p11 <- ggplot(Data_2, aes(x=EPUlag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('EPU')+ labs(fill='Green bubble')
p12 <- ggplot(Data_2, aes(x=Oillag, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Oil price')+ labs(fill='Green bubble')

p13 <- ggplot(Data_2, aes(x=diff_Enii, group=dummy_Renixx, color=dummy_Renixx, fill=dummy_Renixx)) + geom_density(alpha=0.4) +
     guides(color = "none")+ geom_density(alpha=0.4) + xlab('Dfference energy index')+ labs(fill='Green bubble')

grid.arrange(p9, p10, p11, p12,p13, ncol=2,top = textGrob("Renixx Global Index Explosive Evets",gp=gpar(fontsize=20,font=3)))
```

```{r}
Data_2$dummy_Renixx<- as.factor(ifelse(Data_FA$Month >= "2007-07-01" & Data_FA$Month <= "2007-08-01"|Data_FA$Month >= "2007-10-01" & Data_FA$Month <= "2008-01-01"|Data_FA$Month >= "2015-04-01" & Data_FA$Month <= "2015-06-01"|Data_FA$Month >= "2019-12-01" & Data_FA$Month <= "2020-03-01"|Data_FA$Month >= "2020-07-01" & Data_FA$Month <= "2021-05-01"|Data_FA$Month >= "2021-11-01" & Data_FA$Month <= "2021-12-01", 1,0))

Data_2$Time<-scale(c(1:215),center=TRUE,scale=TRUE)
train.data  <- Data_2[c(1:190),]
vali.data <- Data_2[c(191:215),]
#Dumy code categorical predictor variables
x <- model.matrix(dummy_Renixx ~., train.data)[,-1]
#Convert the outcome (class) to a numerical variable
y <- train.data$dummy_Renixx
```

```{r}
set.seed(123)
seeds <- vector(mode = "list", length = 65)
for(i in 1:64) seeds[[i]] <- sample.int(1000, 15)
seeds[[65]] <- sample.int(1000, 1)
tuneLength.num <- 15
myTimeControl <- trainControl(method = "timeslice",
                              initialWindow = 135,
                              horizon = 20,
                              fixedWindow = FALSE,
                              allowParallel = FALSE,
                              seeds = seeds)

glmnet.mod <- train(dummy_Renixx ~.,
                    data = train.data,
                    method = "glmnet",
                    family = "binomial",
                    trControl = c(myTimeControl),
                    tuneLength=tuneLength.num) 
glmnet.mod
```

```{r}
#Fit the final model on the training data
model <- glmnet(x, y, alpha = glmnet.mod$bestTune$alpha, family = "binomial",
                lambda =glmnet.mod$bestTune$lambda)
model_plot <- glmnet(x, y, alpha = glmnet.mod$bestTune$alpha, family = "binomial")
#Display regression coefficients
coef(model)
#Make predictions on the test data
x.test <- model.matrix(dummy_Renixx ~., vali.data)[,-1]
probabilities <- model %>% predict(newx = x.test)
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
#Model accuracy
observed.classes <- vali.data$dummy_Renixx
mean(predicted.classes == observed.classes)
confusionMatrix(as.factor(predicted.classes[,1]), observed.classes)

plot(model_plot, xvar = "lambda", label=T)
lbs_fun <- function(fit, offset_x=1, ...) {
L <- length(fit$lambda)
x <- log(fit$lambda[L])+ offset_x
y <- fit$beta[, L]
labs <- names(y)
text(x, y, labels=labs, ...)
}
```

#Logit model

The logit model is also implemented on the same train/test split to outline the improvements provided by the elastic-net regularization.

```{r}
model_f<-glm(dummy_Renixx ~., family = binomial(link = "logit"), data = Data_2[1:190,c(1:14)])
model_0<-glm(dummy_Renixx ~1, family = binomial(link = "logit"), data = Data_2[1:190,])
model_2<-glm(dummy_Renixx ~MSCIlag+diff_Enii+Energy_ilag+Techlag, family = binomial(link = "logit"), data = Data_2[1:190,])
summary(model_2)
model_2$deviance
-2*logLik(model_2)

vif(model_2)
lrtest (model_0, model_2)
lrtest (model_2, model_f)
blr_linktest(model_2)

y_hat2 <- predict(model_2, type='response')
mean((y_hat2 >= 0.5 & Data_2[,]$dummy_Renixx==0) | (y_hat2 < 0.5 & Data_2[,]$dummy_Renixx==1))
1 - max(prop.table(table(Data_2[,]$dummy_Renixx)))

list(model = pscl::pR2(model_2)["McFadden"])

model1_data <- augment(model_2) %>% mutate(index = 1:n())

ggplot(model1_data, aes(index, .std.resid, color = dummy_Renixx)) + 
  geom_point(alpha = .5) +
  geom_ref_line(h = 3)

or_glm(data = Data_2[1:190,], model = model_2, incr = c(Naturallag=0.1,Green_elag=0.1,Energy_ilag=0.1,Techlag=0.1,Carbon_plag=0.1,Warminglag=0.1,Energy_slag=0.1,Taxlag=0.1,MSCIlag=0.1,diff_Enii=0.1,Time=0.1,OFRlag=0.1))
```

```{r}
predictions <- predict(model_2, type = "response", newdata=Data_2[191:215,])
predictions

threshold <- 0.5
binary_predictions <- ifelse(predictions > threshold, 1, 0)
confusionMatrix(as.factor(binary_predictions), observed.classes)
```

#VAR

In the following we will investigate if the proposed covariates have a role to predict future market values of the Renixx index. In this regard, the Granger causality test is performed to outline the existence of possible causality effects

```{r}
#granger causality
Finance$Merge<-format(as.Date(Finance$Month), "%Y-%m")
Climate$Merge<-format(as.Date(Climate$Date), "%Y-%m")

Data_3<-merge(Finance,Climate,by="Merge")
Data_3<- Data_3[c(1:216),-c(1,9)]
rownames(Data_3) <- NULL

grangertest(MSCI ~ Renixx, order =12, data = Data_3)
grangertest(MSCI ~ Oil_p, order =12, data = Data_3)#Granger
grangertest(MSCI ~ Geo_i, order =12, data = Data_3)
grangertest(MSCI ~ Energy_i, order =12, data = Data_3)
grangertest(MSCI ~ Green_e, order =12, data = Data_3)
grangertest(MSCI ~ Tech, order =12, data = Data_3)
grangertest(MSCI ~ Warming, order =12, data = Data_3)
grangertest(MSCI ~ Natural, order =12, data = Data_3)
grangertest(MSCI ~ Carbon_p, order =12, data = Data_3)
grangertest(MSCI ~ Tax, order =12, data = Data_3)
grangertest(MSCI ~ Energy_s, order =12, data = Data_3)
grangertest(MSCI ~ EPU, order =12, data = Data_3)
grangertest(MSCI ~ OFR, order =12, data = Data_3)

grangertest(Renixx ~ MSCI, order = 12, data = Data_3)
grangertest(Renixx ~ Oil_p, order = 12, data = Data_3)#Granger
grangertest(Renixx ~ Geo_i, order = 12, data = Data_3)
grangertest(Renixx ~ EPU, order = 12, data = Data_3)#Granger
grangertest(Renixx ~ OFR, order = 12, data = Data_3)
grangertest(Renixx ~ Energy_i, order = 12, data = Data_3)
grangertest(Renixx ~ Green_e, order = 12, data = Data_3)
grangertest(Renixx ~ Tech, order = 12, data = Data_3)
grangertest(Renixx ~ Warming, order = 12, data = Data_3)
grangertest(Renixx ~ Natural, order = 12, data = Data_3)
grangertest(Renixx ~ Carbon_p, order = 12, data = Data_3)
grangertest(Renixx ~ Energy_s, order = 12, data = Data_3)
grangertest(Renixx ~ Tax, order = 12, data = Data_3)#Granger

grangertest(Oil_p ~ MSCI, order = 12, data = Data_3)#Granger
grangertest(Oil_p ~ Natural, order = 12, data = Data_3)
grangertest(Oil_p ~ Geo_i, order = 12, data = Data_3)
grangertest(Oil_p ~ Renixx, order = 12, data = Data_3)
grangertest(Oil_p ~ Green_e, order = 12, data = Data_3)
grangertest(Oil_p ~ Tech, order = 12, data = Data_3)
grangertest(Oil_p ~ Warming, order = 12, data = Data_3)
grangertest(Oil_p ~ Energy_i, order = 12, data = Data_3)
grangertest(Oil_p ~ Carbon_p, order = 12, data = Data_3)
grangertest(Oil_p ~ EPU, order = 12, data = Data_3)
grangertest(Oil_p ~ OFR, order = 12, data = Data_3)#Granger

grangertest(OFR ~ MSCI, order = 12, data = Data_3)#Granger
grangertest(OFR ~ Oil_p, order = 12, data = Data_3)#Granger
grangertest(OFR ~ Geo_i, order = 12, data = Data_3)
grangertest(OFR ~ Renixx, order = 12, data = Data_3)#Granger
grangertest(OFR ~ Green_e, order = 12, data = Data_3)
grangertest(OFR ~ Tech, order = 12, data = Data_3)
grangertest(OFR ~ Warming, order = 12, data = Data_3)
grangertest(OFR ~ Natural, order = 12, data = Data_3)
grangertest(OFR ~ Carbon_p, order = 12, data = Data_3)
grangertest(OFR ~ Tax, order = 12, data = Data_3)#Granger
grangertest(OFR ~ EPU, order = 12, data = Data_3)
grangertest(OFR ~ Energy_i, order = 12, data = Data_3)
grangertest(OFR ~ Energy_s, order = 12, data = Data_3)
```

```{r}
Data_or$Energy_n<-scale(Energy_i$Absolute.Google.Search.Volume+Energy_s$Absolute.Google.Search.Volume,center=TRUE,scale=TRUE)
ggplot() + 
     geom_line(data = Data_or, aes(x = Month, y = Renixx, color = "Renixx")) +
     geom_line(data = Data_or, aes(x = Month, y = Energy_i, color = 'Energy Index'))+
  geom_line(data = Data_or, aes(x = Month, y = Energy_n, color = 'Energy New'))+
  xlab('Date') + ggtitle('Series')+
  ylab('Monthly series')+ scale_color_manual(name='Series',
                     breaks=c('Renixx', 'Energy Index', 'Energy New'),
                     values=c('Renixx'='red', 'Energy Index'='blue','Energy New'='darkgreen'))

cor(Data_or$Renixx,Data_or$Energy_i)
cor(Data_or$Energy_i[121:216],Data_or$Renixx[121:216])
cor(Data_or$Energy_s[121:216],Data_or$Renixx[121:216])
cor(Data_or$Oil_p[121:216],Data_or$Renixx[121:216])
```

```{r}
Data_VECM<-as.data.frame(log(renixx_m$close))
colnames(Data_VECM)[1]<-'Renixx'

Data_VECM$Energy_i<-log(Energy_i$Absolute.Google.Search.Volume)

Data_VECM$Energy_s<-log(Energy_s$Absolute.Google.Search.Volume)

Data_VECM$Oil_p<-log(Oil_m$Price)
```

#2015 Jan.

Given the importance of the year 2015 in related-climate change events, we restrict the dataset on observations between January 2015 and December 2022. Then, a VECM model is estimated with the variables turned out to be significant by the Granger causality test.

```{r}
grangertest(Renixx ~ Energy_i, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Energy_s, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Oil_p, order = 12, data = Data_3[121:216,])#Granger
#Granger
grangertest(Energy_s ~ Renixx, order = 12, data = Data_3[121:216,])
grangertest(Energy_i ~ Renixx, order = 12, data = Data_3[121:216,])
grangertest(Oil_p ~ Renixx, order = 12, data = Data_3[121:216,])

grangertest(Renixx ~ MSCI, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Tax, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Oil_p, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Geo_i, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Green_e, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Tech, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Warming, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Natural, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Carbon_p, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ EPU, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ OFR, order = 12, data = Data_3[121:216,])
grangertest(Renixx ~ Energy_i, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Energy_s, order = 12, data = Data_3[121:216,])#Granger

grangertest(Renixx ~ Energy_i, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Energy_s, order = 12, data = Data_3[121:216,])#Granger
grangertest(Renixx ~ Oil_p, order = 12, data = Data_3[121:216,])#Granger

grangertest(Energy_s ~ Renixx, order = 12, data = Data_3[121:216,])
grangertest(Energy_i ~ Renixx, order = 12, data = Data_3[121:216,])
grangertest(Oil_p ~ Renixx, order = 12, data = Data_3[121:216,])

grangertest(Energy_s ~ Energy_i, order = 12, data = Data_3[121:216,])
grangertest(Energy_s ~ Oil_p, order = 12, data = Data_3[121:216,])
grangertest(Oil_p ~ Energy_i, order = 12, data = Data_3[121:216,])
grangertest(Oil_p ~ Energy_s, order = 12, data = Data_3[121:216,])
grangertest(Energy_i ~ Energy_s, order = 12, data = Data_3[121:216,])
grangertest(Energy_i ~ Oil_p, order = 12, data = Data_3[121:216,])

all_series<-ts(Data_VECM[c(121:216),c('Renixx','Energy_i','Oil_p',"Energy_s")],start=c(2015,1), end=c(2022,12),frequency=12)

var <- VARselect(all_series, type = "both", lag.max = 12, season = 12)
var$selection["FPE(n)"]

var_all = VAR(all_series,5, season = 12,type = 'both') #3 for cointegration
summary(var_all)
causality(var_all, cause = "Renixx")
causality(var_all, cause = "Energy_i")
causality(var_all, cause = "Energy_s")
causality(var_all, cause = "Oil_p")
```

```{r}
res<-matrix(c(var_all[["varresult"]][["Renixx"]][["residuals"]],var_all[["varresult"]][["Energy_i"]][["residuals"]],var_all[["varresult"]][["Energy_s"]][["residuals"]],var_all[["varresult"]][["Oil_p"]][["residuals"]]), ncol = 4)
mvshapiro_test(res)
vars::arch.test(var_all,multivariate.only = TRUE)   
normality.test(var_all,multivariate.only = TRUE)  
serial.test(var_all, type = "PT.asymptotic",lags.pt = 12)
plot(stability(var_all), nc = 2)

impresp <- irf(var_all)
plot(impresp)

fevd(var_all)
```

```{r}
nlags=5-1
jotest_all=ca.jo(all_series, type="eigen", K=nlags, ecdet="trend", spec= "transitory", season = 12)
summary(jotest_all)
jotest_all=ca.jo(all_series, type="trace", K=nlags, ecdet="trend", spec= "transitory", season = 12)
summary(jotest_all)
```

```{r}
lttest(jotest_all, r=1)

y.VEC <- cajorls(jotest_all, r=1)
y.VEC
summary(y.VEC$rlm)

urca::plotres(jotest_all)
v0.VAR <- vec2var(jotest_all, r = 1)
v0.VAR

impresp <- irf(v0.VAR)
plot(impresp)

fevd(v0.VAR)$Renixx
```

Successively, both an ARIMA and an ARIMAX model are estimated to outline the most suitable model to forecasting future values of th Renixx index.

#Arimax eni

```{r}
all_series<-ts(Data_VECM[c(121:216),c('Renixx','Energy_i','Oil_p','Energy_s')],start=c(2015,1), end=c(2022,12),frequency=12)

arimax_eni<-auto.arima(all_series[,1],seasonal = TRUE, xreg=all_series[,c('Energy_i','Energy_s','Oil_p')])
arimax_eni
arimax_1<-Arima(all_series[,1], order = c(1,0,1), seasonal = list(order = c(0, 0, 0), period = 12),xreg =all_series[,c('Energy_i','Energy_s','Oil_p')])
summary(arimax_1)
coeftest(arimax_1)
```

```{r}
#Function
ncvTest.arima <- function(model, ...) {
    data <- getCall(model)$data
    model <- model
    sumry <- summary(model)
    residuals <- residuals(model)

    S.sq <- (length(residuals)-2)*(model$sigma2)/length(residuals)
    #cat("S.sq=",S.sq,"\n")
    .U <- (residuals^2)/S.sq
    #cat(".U=",.U,"\n")
    mod <- lm(.U ~ fitted.values(model))
    varnames <- "fitted.values"
    var.formula <- ~ fitted.values
    df <- 1
    SS <- anova(mod)$"Sum Sq"
    #cat("SS=",SS,"\n")
    RegSS <- sum(SS) - SS[length(SS)]
    Chisq <- RegSS/2
    result <- list(formula=var.formula, formula.name="Variance",ChiSquare=Chisq, Df=df, p=pchisq(Chisq, df, lower.tail=FALSE), test="Non-constant Variance Score Test")
    class(result) <- "chisqTest"
    result
} 
```


```{r}
#Residuals
res<-arimax_1$residuals
plot(res)
McLeod.Li.test(object=arimax_1)
checkresiduals(arimax_1)
JarqueBera.test(arimax_1)
ncvTest.arima(arimax_1)
```
An EGARCH model with hyperbolic distribution is proposed to deal with the existence of heteroskedasticity in the residuals of the ARIMAX model. So the final model is a bombined ARIMAX-EGARCH.

```{r}
spec <- ugarchspec(variance.model = list(model = "eGARCH",garchOrder = c(1, 1),external.regressors =matrix(all_series[,c('Energy_i','Energy_s','Oil_p')],ncol=3)),
mean.model=list(armaOrder=c(1,1),external.regressors =matrix(all_series[,c('Energy_i','Energy_s','Oil_p')],ncol=3),arfima=TRUE ),distribution.model="ghyp",fixed.pars=list(arfima = 0))
arimax_enigarch <- ugarchfit(spec = spec, data =all_series[,1],solver.control = list(trace=0),xreg=all_series[,c('Energy_i','Energy_s','Oil_p')])
arimax_enigarch
```

#Arima

The ARIMA model is treated as the baseline model, so no further developments are considered.

```{r}
all_series<-ts(Data_VECM[c(121:216),'Renixx'],start=c(2015,1), end=c(2022,12),frequency =12)
arima_n<-auto.arima(all_series,seasonal = TRUE)
arima_n
arima<-Arima(all_series, order = c(1,1,1), seasonal = list(order = c(0, 0, 0), period = 12))
summary(arima)
```

```{r}
#Residuals
res<-arima$residuals
plot(res)
McLeod.Li.test(object=arima)
checkresiduals(arima)
JarqueBera.test(arima)
ncvTest.arima(arima)
```

```{r}
#spec <- ugarchspec(variance.model = list(model = "sGARCH",garchOrder = c(1, 1)),
#mean.model=list(armaOrder=c(1,1),arfima=TRUE ),distribution.model="ghyp",fixed.pars=list(arfima = 1))
#arimagarch <- ugarchfit(spec = spec, data =all_series,solver.control = list(trace=0))
#arimagarch
```

```{r}
Energy_i <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Energy_index.csv", header=TRUE, comment.char="#")

Energy_1<-log(Energy_i$Absolute.Google.Search.Volume[229:234])
Energy_s <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Google Trends/Energy_shares.csv", header=TRUE, comment.char="#")
Energy_2<-log(Energy_s$Absolute.Google.Search.Volume[229:234])
Oil_pred <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Predictions/Oil_pred.csv", header=TRUE, comment.char="#")
Oil_pred$Date<-mdy(Oil_pred$Date)
df_p <- Oil_pred %>% dplyr::select(1,2)
Month<- as.Date(cut(df_p$Date, "month"))
Oil_pm <- aggregate(Price ~ Month, df_p, mean)
Oil_pm<-log(Oil_pm$Price)
Exo<-data.frame(c(Energy_1),c(Oil_pm))
colnames(Exo)[1]<-'Energy_i'
colnames(Exo)[2]<-'Oil_p'
```

#Predictions

All models are tested considering both the MAPE and the RMSE on the next observed values of the Renixx index up until July 2023.

```{r}
garch_eni = ugarchforecast(arimax_enigarch, n.ahead = 6, external.forecasts=list(mregfor = matrix(c(Energy_1,Energy_2,Oil_pm),ncol=3), vregfor = matrix(c(Energy_1,Energy_2,Oil_pm),ncol=3)))
print(garch_eni)
plot(garch_eni)
as.numeric(garch_eni@forecast[["seriesFor"]])

#garch_arima = ugarchforecast(arimagarch, n.ahead = 6)
#print(garch_arima)
#plot(garch_arima)
#as.numeric(garch_arima@forecast[["seriesFor"]])
```

```{r}
pred_varall<- predict(v0.VAR, n.ahead = 6)
autoplot(pred_varall)+ facet_wrap(~variable, ncol = 1, scales = "free_y")

#pred_arimaeni<-forecast::forecast(arimax_eni, h = 6,xreg=matrix(c(Energy_1,Energy_2,Oil_pm),ncol=3))
#autoplot(pred_arimaeni)

pred_arima<-forecast::forecast(arima_n, h = 6)
autoplot(pred_arima)
```

```{r}
renixx_p <- read.csv("~/GitHub/PhD-Thesis/Green bubbles/Predictions/renixx_pred.csv")

#DESCRIPTIVE STATISTICS

renixx_p$date<-ymd(renixx_p$date)
ggplot(renixx_p, aes(date, close))+geom_line()+ggtitle('Renixx index future values')+xlab('Time')+ylab('Absolute Returns')
```

```{r}
renixx_p$date<-ymd(renixx_p$date)
df_p <- renixx_p %>% dplyr::select(1,3)
Month<- as.Date(cut(df_p$date, "month"))
renixx_pm <- aggregate(close ~ Month, df_p, mean)
renixx_pm$close<-log(renixx_pm$close)

Metrics::rmse(renixx_pm$close,pred_varall$fcst$Renixx[,1])
Metrics::rmse(renixx_pm$close,pred_arima$mean)
Metrics::rmse(renixx_pm$close,as.numeric(garch_eni@forecast[["seriesFor"]]))

mape(renixx_pm$close,pred_varall$fcst$Renixx[,1])
mape(renixx_pm$close,pred_arima$mean)
mape(renixx_pm$close,as.numeric(garch_eni@forecast[["seriesFor"]]))
```

#Bayesian

A Bayesian framework to may considered on the overall sample size relaxing the assumption of constant predictors.

```{r}
set.seed(1)

mn <- bv_minnesota(lambda = bv_lambda(mode = 0.2, sd = 0.4, min = 0.0001, max = 5),alpha = bv_alpha(mode = 2), var = 1e07)

soc <- bv_soc(mode = 1, sd = 1, min = 1e-04, max = 50)

sur <- bv_sur(mode = 1, sd = 1, min = 1e-04, max = 50)

priors <- bv_priors(hyper = "auto", mn = mn, soc = soc, sur = sur)

mh <- bv_metropolis(scale_hess = c(0.05, 0.0001, 0.0001),adjust_acc = TRUE, acc_lower = 0.25, acc_upper = 0.45)

all_series<-ts(Data_VECM[c(1:216),c('Renixx','Energy_s','Energy_i','Oil_p')],start=c(2005,1), end=c(2022,12),frequency =12)

max_lag <- 10

# Create an empty matrix to store information criteria values
ic_values <- matrix(NA, nrow = max_lag, ncol = 1)

# Loop through different lag orders and calculate BIC
for(lag_order in 1:max_lag) {
  bvar_model <- BVAR<-bvar(all_series, lags =lag_order, n_draw = 200000, n_burn = 50000, n_thin = 1,priors = priors, mh = mh, verbose = TRUE)
  ic_values[lag_order, 1] <- summary(bvar_model)$logLik[1]
}

# Find the lag order with the minimum LL
selected_lag_order <- which.max(ic_values)

# Print the selected lag order
print(selected_lag_order)

BVAR<-bvar(all_series, lags = selected_lag_order, n_draw = 200000, n_burn = 50000, n_thin = 1,priors = priors, mh = mh, verbose = TRUE)
```

```{r}
print(BVAR)
plot(BVAR)
plot(BVAR, type = "dens",vars_response = "Renixx", vars_impulse = "Energy_i")
plot(residuals(BVAR, type = "mean"))

opt_irf <- bv_irf(horizon = 12, identification = TRUE)
irf(BVAR) <- irf(BVAR, opt_irf, conf_bands = c(0.05, 0.1))
plot(irf(BVAR), area = TRUE,vars_impulse = c("Oil_p",'Energy_i','Energy_s'), vars_response = c('Renixx'))
```

```{r}
predict_BVAR <- predict(BVAR, horizon = 6, conf_bands = c(0.05, 0.1))
plot(predict_BVAR, area = TRUE, t_back = 32)
pred_BVAR<-predict_BVAR[["quants"]][3,,1]

Metrics::rmse(renixx_pm$close,pred_BVAR)
mape(renixx_pm$close,pred_BVAR)
```

#Finance implications

In terms of Climate Minsky moments we outline the possibility of such a phenomena through rapid changes in the Renixx index. Performin a VAR model on the overall sample a same evidence is not outlined by changes in oil prices.

```{r}
all_series<-ts(Data_3[c(1:216),c('Renixx','OFR','Oil_p')],start=c(2005,1), end=c(2022,12),frequency=12)

var <- VARselect(all_series, type = "none", lag.max = 12, season = 12)
var$selection["FPE(n)"]

var_OFR <- VAR(all_series,1, season = 12,type = 'none')
#summary(fitvar1)
summary(var_OFR)
causality(var_OFR, cause = "OFR")
causality(var_OFR, cause = "Renixx")
causality(var_OFR, cause = "Oil_p")
#causality(var_OFR, cause = "MSCI")
#causality(var_OFR, cause = "Tax")
xtable(var_OFR$varresult$OFR)
```

```{r}
vars::arch.test(var_OFR,multivariate.only = TRUE)   
normality.test(var_OFR,multivariate.only = TRUE)  
serial.test(var_OFR, type = "BG",lags.bg = 12)
plot(stability(var_OFR), nc = 2)

impresp <- irf(var_OFR,impulse = 'Renixx', response = 'OFR')
impresp$irf$Renixx <- -impresp$irf$Renixx #negative shock
impresp$Lower$Renixx <- -impresp$Lower$Renixx
impresp$Upper$Renixx <- -impresp$Upper$Renixx
plot(impresp, main='Impulse Response Function from RENIXX, negative shock',ylab='FSI')

plot(fevd(var_OFR),nr = 2)
fevd(var_OFR)
```

```{r}
#https://rpubs.com/Pun_/Exploratory_factor_Analysis
#Promax Rotation. An oblique rotation, which allows factors to be correlated. This rotation can be calculated more quickly than a direct oblimin rotation, so it is useful for large datasets.

#Data_FA<-renixx_m[,c(1,4)]
#colnames(Data_FA)[2] <- "Renixx"
#Data_FA$Oil_p<-Finance$Oil_p
#Data_FA$MSCI<-Finance$MSCI
#Data_FA$EPU<-Finance$EPU
#Data_FA$Geo_i<-Finance$Geo_i
#Data_FA$OFR<-Finance$OFR
#
#Data_FA$Carbon_p<-Carbon_p[,4]
#Data_FA$Energy_i<-Energy_i[,4]
#Data_FA$Green_e<-green_e[,4]
#Data_FA$Tax<-Tax[,4]
#Data_FA$Energy_s<-Energy_s[,4]
#
#Data_FA<-Data_FA[c(2:216),]
#rownames(Data_FA)<-NULL
#
#KMO(Data_FA[,c(3:length(Data_FA[1,]))])
#Data_factor <-Data_FA[,c(3:length(Data_FA[1,]))]
#Data_factor <-Data_factor[,KMO(Data_factor)$MSAi>0.5]
#base::round(KMO(Data_factor)$MSA,2)
#
#cortest.bartlett(Data_factor)
#
#ev <- eigen(cor(Data_factor)) # get eigenvalues
#ev$values
#
#scree(Data_factor, pc=FALSE)
#
#fa.parallel(Data_factor)
#
#Nfacs <- 2 # This is for four factors. You can change this as needed.
#
#fit <- fa(Data_factor, Nfacs, rotate = "none",fm="promax")
#
#print(fit, digits=3, cutoff=0.3, sort=TRUE)
#
#loads <- fit$loadings
#
#fa.diagram(loads)
#
#dim(fit$loadings)
#
#fs <- factor.scores(Data_factor, fit)   
#fs <- fs$scores
#Data_FA$Factor_climate<-fs[,2]
#Data_FA$Factor_finance<-fs[,1]
#
#Data_FA$Renixx<-Finance$Renixx[2:216]
#
#var <- VARselect(Data_FA[c(121:215),c('Renixx','Factor_finance','Factor_climate')], type = "const", #lag.max = 12, season = 12)
#var$selection["FPE(n)"]
#
#var_fact<-VAR(Data_FA[c(1:215),c('Renixx','Factor_finance','Factor_climate')],season = #12,p=1,type='const')
#
#summary(var_fact)
#causality(var_fact, cause = 'Factor_climate')
#causality(var_fact, cause = 'Renixx')
#causality(var_fact, cause = 'Factor_finance')
```

#GRAPHS

```{r}
#par(mfrow = c(2, 1))
#plot(renixx_m$Month, renixx_m$close, type = "l", xlab = "", ylab = "", bty = "l",col='blue',main='Close #prices', adj = 0)
#abline(v = breack, lty = 2, col = 'red')
#plot(renixx_m$Month, renixx_m$abs, type = "l", xlab = "", ylab = "", bty = #"l",col='blue',main='Absolute log-returns', adj = 0)
#abline(v = breack, lty = 2, col = 'red')
```


#THE END
